# Iniciativa 001: Segmenta√ß√£o de Cliente

## Informa√ß√µes Gerais

| Campo | Descri√ß√£o |
|-------|------------|
| **Nome** | Segmenta√ß√£o de cliente |
| **√Årea de Neg√≥cio** | Pricing |
| **Data de In√≠cio** | 14/10/2025 |
| **Data Prevista de Finaliza√ß√£o** | 15/10/2025 |
| **Status Atual** | Em especifica√ß√£o |

---

## 1. Contexto e Impacto

### Problema / Desafio Atual
- N√£o existe prioriza√ß√£o de atendimento entre clientes de alto e baixo valor.  
- As an√°lises atuais s√£o manuais e demoradas, dificultando decis√µes estrat√©gicas.  
- H√° baixa personaliza√ß√£o no relacionamento com clientes e oportunidades perdidas de upsell.  
- Clientes com pouca atividade tendem ao churn, sem monitoramento preventivo.  

### Situa√ß√£o Atual
- Todos os clientes s√£o tratados da mesma forma, sem segmenta√ß√£o baseada em valor, frequ√™ncia ou margem.

### Objetivo
- Criar segmenta√ß√£o de clientes baseada em comportamento transacional, rentabilidade e frequ√™ncia de uso.  
- Identificar clientes mais valiosos (Top 20/50) e clientes em risco (Bottom 50) para personaliza√ß√£o de atendimento e aumento de rentabilidade.

### Impacto Esperado
- Aumento de receita com base em clientes priorit√°rios.  
- Diminui√ß√£o do churn com a√ß√µes proativas.  
- Melhoria no LTV (Lifetime Value) e na lucratividade total da carteira.

### KPIs / Indicadores Alvo
- Redu√ß√£o de churn (%)  
- Aumento do LTV m√©dio (R$)  
- Crescimento da receita de taxa  
- Maior frequ√™ncia m√©dia de transa√ß√µes (√∫ltimos 30 dias)  
- Aumento do ticket m√©dio de transa√ß√£o  

---

## 2. Pontos de Aten√ß√£o

| Ponto | Data de Identifica√ß√£o | A√ß√£o Necess√°ria | Respons√°vel | Status |
|-------|-----------------------|-----------------|--------------|--------|
| Padronizar cliente_id e customer_id | 14/10 | Uniformizar nomenclatura em toda a pipeline | Engenharia de Dados | Em andamento |
| Convers√£o de moeda | 14/10 | Normalizar pre√ßos e taxas para USD | Engenharia de Dados | Pendente |
| Privacidade de dados pessoais | 14/10 | Aplicar hash SHA2 e RLS no Databricks | Seguran√ßa / Dados | Em andamento |
| Categorias v√°lidas (segmento, estado, pa√≠s) | 14/10 | Validar dom√≠nio permitido | QA | Em andamento |
| Quantidade e pre√ßo v√°lidos | 14/10 | Garantir valores positivos e tipos num√©ricos | QA | Em andamento |

---

## 3. Escopo T√©cnico

- **Ferramenta:** Lakeflow (Databricks SQL)  
- Toda a modelagem ser√° feita em SQL declarativo, com governan√ßa no **Unity Catalog**.  
- Monitoramento e data quality nativos via **EXPECT/CONSTRAINT**.  
- **Frequ√™ncia:** execu√ß√£o a cada 1 hora (H+00) via **Lakeflow Scheduler**.  
- **SLA:** publica√ß√£o da camada Gold at√© H+15 minutos.  

---

## 4. Arquitetura e Camadas

### ü™∂ **Camada Silver ‚Äî 4 Tabelas Principais**

> ‚öôÔ∏è **Observa√ß√£o importante:**  
> Na **camada Silver** √© o momento ideal para **fazer os *castings* de tipo** ‚Äî convertendo colunas de data, n√∫meros e texto para os formatos corretos (`DATE`, `TIMESTAMP`, `DECIMAL`, `STRING` etc).  
> Isso garante **padroniza√ß√£o**, **performance nas jun√ß√µes** e **qualidade dos c√°lculos** nas camadas Gold.

| Tabela | Descri√ß√£o | Regras de Qualidade |
|---------|------------|----------------------|
| **fact_transaction_assets** | Uni√£o de transa√ß√µes BTC e Commodities, normaliza√ß√£o de `asset_symbol` e `quantidade`. | `quantidade > 0`, `tipo_operacao` v√°lido, `data_hora` n√£o nula |
| **fact_quotation_assets** | Uni√£o de cota√ß√µes BTC e yFinance; padroniza√ß√£o de ativo, pre√ßo e moeda. | `preco` convertido para DECIMAL, `preco > 0`, `timestamp` v√°lido |
| **dim_clientes** | C√≥pia anonimizada da base de clientes; valida√ß√£o de categorias. | `documento` anonimizado via SHA2, `segmento/pais/estado` v√°lidos, `customer_id` √∫nico |
| **fact_transaction_revenue** | Jun√ß√£o entre transa√ß√µes e cota√ß√µes, c√°lculo de valor e receita de taxa (camada Silver de enriquecimento antes da Gold). | `gross_value = quantidade √ó pre√ßo`, `fee_revenue = gross_value √ó 0.25%`, cota√ß√£o v√°lida |

---

### üèÜ **Camada Gold**

| Tabela | Descri√ß√£o | Regras de Qualidade |
|---------|------------|----------------------|
| **mostvaluableclient** | Agrega√ß√£o por cliente: total, m√©dia, frequ√™ncia, ticket m√©dio e ranking. | `transa√ß√µes ‚â• 1`, m√©tricas consistentes |

---

## 5. Regras de Qualidade de Dados (Data Quality)

### 5.1 fact_transaction_assets
- `quantidade > 0`  
- `data_hora` n√£o nula  
- `tipo_operacao IN ('COMPRA','VENDA')`  
- `asset_symbol` padronizado  
- **A√ß√£o:** registros inv√°lidos enviados para `dq_quarantine.fact_transaction_assets`

### 5.2 fact_quotation_assets
- `preco` convertido para `DECIMAL(18,4)`  
- `preco > 0`  
- `horario_coleta <= current_timestamp()`  
- `ativo` dentro do dom√≠nio permitido  
- **A√ß√£o:** registros inconsistentes enviados para `dq_quarantine.fact_quotation_assets`

### 5.3 dim_clientes
- `documento_hash = SHA2(documento, 256)`  
- `segmento ‚àà {Financeiro, Ind√∫stria, Varejo, Tecnologia}`  
- `estado ‚àà siglas BR + (DE, US)`  
- `pais ‚àà {Brasil, Alemanha, Estados Unidos}`  
- `customer_id` √∫nico  
- **A√ß√£o:** campos nulos ou fora de dom√≠nio enviados para `dq_quarantine.dim_clientes`

### 5.4 fact_transaction_revenue
- Cada transa√ß√£o deve possuir cota√ß√£o v√°lida ‚â§ `ts`  
- `gross_value > 0`  
- `fee_revenue > 0`  
- `customer_sk` n√£o nulo (join v√°lido com `dim_clientes`)  
- **A√ß√£o:** inconsist√™ncias enviadas para `dq_quarantine.fact_transaction_revenue`

### 5.5 mostvaluableclient
- **M√©tricas:**  
  - `COUNT(transa√ß√µes)`  
  - `SUM(valor_total)`  
  - `AVG(ticket_m√©dio)`  
  - `MIN/MAX(data_transacao)`  
  - Frequ√™ncia m√©dia √∫ltimos 30 dias  
  - `MAX(comiss√£o)`  
- **Ranking:** Top 20 / Top 50 / Bottom 50  
- **A√ß√£o:** reprocessamento autom√°tico se viola√ß√£o de DQ > 1%

---

## 6. Anonimiza√ß√£o e Seguran√ßa

| Tipo | T√©cnica | Implementa√ß√£o |
|------|----------|----------------|
| **PII (documentos, nomes, endere√ßos)** | Hash (SHA2 256) | `SHA2(documento, 256)` aplicado em `dim_clientes` |
| **Acesso a dados sens√≠veis** | Row Level Security (RLS) | Implementado via Lakeflow / Databricks SQL |
| **Governan√ßa** | Unity Catalog | Grants por camada: leitura apenas na Gold |
| **Auditoria e Linhagem** | Lakeflow Lineage | Rastreabilidade completa dos fluxos |

---

## 7. Volumes do CSV

Os arquivos **Bronze** foram montados no cat√°logo `lakehouse` como **volumes**, e servir√£o como origem para o **DLT copiar do RAW para a camada Bronze**.  
Esses dados precisam ser lidos utilizando a fun√ß√£o `cloud_files()` para ingest√£o incremental.

### üìÅ Caminhos dos Volumes

