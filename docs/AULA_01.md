# üìö AULA 01: Fundamentos do Lakehouse na Pr√°tica ‚Äî Ingest√£o, Agendamento e DLT

## üéØ Objetivo da Aula
Construir, agendar e operacionalizar ingest√µes de dados a partir de 2 fontes de APIs (Bitcoin e commodities: ouro, prata e petr√≥leo) e 1 fonte relacional (Supabase), simulando um data lake com camadas raw‚Üíbronze usando DLT.

---

## ‚è∞ Dura√ß√£o Estimada
**3-4 horas** (incluindo exerc√≠cios pr√°ticos)

---

## üìã Pr√©-requisitos
- [ ] Conta no GitHub criada
- [ ] Conhecimentos b√°sicos de SQL (recomendado)
- [ ] Conhecimentos b√°sicos de Python (opcional)
- [ ] Navegador web atualizado
- [ ] Conta no Supabase (para o banco relacional)

---

## üó∫Ô∏è Roteiro da Aula

### Parte 1: Introdu√ß√£o ao Databricks (20-30 min)

#### 1.1 O que √© Databricks?
- [ ] Conceito de Lakehouse e evolu√ß√£o dos data warehouses
- [ ] Plataforma unificada: Analytics + AI + Data Engineering
- [ ] Benef√≠cios: performance, escalabilidade, governan√ßa

#### 1.2 Arquitetura Lakehouse
- [ ] Data Warehouse vs Data Lake vs Lakehouse
- [ ] Componentes: Spark, Delta Lake, Unity Catalog
- [ ] Benef√≠cios: ACID, versionamento, governan√ßa

### Parte 2: Configura√ß√£o do Ambiente (30-45 min)

#### 2.1 Criando Conta no Databricks
- [ ] Acesso ao Community Edition
- [ ] Registro e configura√ß√£o inicial
- [ ] Navega√ß√£o: Workspace, clusters, notebooks

#### 2.2 Integra√ß√£o com GitHub
- [ ] Configurar Git Integration
- [ ] Conectar reposit√≥rio e sincronizar (clone/pull/push)
- [ ] Fluxo de branches (main/desenvolvimento)

#### 2.3 Organiza√ß√£o do projeto neste reposit√≥rio
- [ ] `src/ingestao/`: notebooks base de ingest√£o (bitcoin, yfinance/commodities)
- [ ] `src/sql/`: scripts SQL (tabelas e cargas simuladas)
- [ ] `src/bronze/`: notebooks DLT para camadas bronze
- [ ] `dlt_bronze_bitcoin_pipeline-(1)/transformations/`: SQLs DLT

---

## üíæ Parte 3: Ingest√£o via APIs (40-60 min)

#### 3.1 Fontes e escopo
- [ ] Bitcoin: pre√ßo/volume
- [ ] Commodities: ouro, prata e petr√≥leo (yfinance)

#### 3.2 Implementa√ß√£o (usando notebooks como base para scripts)
- [ ] Revisar `src/ingestao/ingest_bitcoin_to_volume.ipynb`
- [ ] Revisar `src/ingestao/ingest_yfinance_to_volume.ipynb`
- [ ] Converter para scripts Python execut√°veis e salvar em `src/ingestao/`
- [ ] Definir diret√≥rios de sa√≠da para camada raw (simulando data lake)

Exemplo de pontos-chave nos scripts:
- Extra√ß√£o da API (requests/yfinance)
- Normaliza√ß√£o de colunas e carimbo de tempo (ingestion_time)
- Escrita em formato parquet/CSV na √°rea raw

#### 3.3 Agendamento
- [ ] Agendar 2 scripts (bitcoin e commodities)
- [ ] Frequ√™ncia: a cada 10 minutos (cron local, crontab, ou Databricks Jobs)
- [ ] Log m√≠nimo: timestamp da execu√ß√£o e quantidade de registros gravados

---

## üóÑÔ∏è Parte 4: Ingest√£o de Banco SQL (Supabase) (30-45 min)

#### 4.1 Cria√ß√£o do Banco
- [ ] Criar projeto no Supabase (Postgres habilitado)
- [ ] Executar `src/sql/create_table.sql`
- [ ] Popular base inicial com `src/sql/seed_customer.sql`

#### 4.2 Cargas e rotinas de atualiza√ß√£o (simula√ß√£o de transa√ß√µes)
- [ ] Executar `src/sql/cron_sales_btc.sql` (transa√ß√µes BTC)
- [ ] Executar `src/sql/cron_sales_commodities.sql` (transa√ß√µes commodities)
- [ ] Agendar 2 rotinas a cada 10 minutos

Observa√ß√£o: manter credenciais seguras e vari√°veis de ambiente fora do c√≥digo.

---

## ü™Ñ Parte 5: DLT ‚Äî Raw para Bronze (30-45 min)

#### 5.1 Vis√£o Geral do DLT
- [ ] O que √© Delta Live Tables e quando usar
- [ ] Boas pr√°ticas de camadas raw‚Üíbronze

#### 5.2 Implementa√ß√£o
- [ ] Utilizar `src/bronze/dlt_bronze_bitcoin.ipynb` (quando aplic√°vel)
- [ ] Referenciar SQLs em `dlt_bronze_bitcoin_pipeline-(1)/transformations/`
- [ ] Criar tabelas bronze a partir dos arquivos raw (bitcoin e commodities)

#### 5.3 Execu√ß√£o e valida√ß√£o
- [ ] Rodar pipeline DLT
- [ ] Verificar lineage e qualidade b√°sica
- [ ] Consultar tabelas bronze

---

## üéØ Objetivos de Aprendizagem

Ao final desta aula, voc√™ ser√° capaz de:

- [ ] Implementar ingest√µes a partir de APIs (bitcoin e commodities)
- [ ] Agendar execu√ß√µes recorrentes (cron/Jobs) a cada 10 minutos
- [ ] Configurar um banco relacional no Supabase e executar cargas SQL
- [ ] Organizar dados em camadas (raw e bronze) no Lakehouse
- [ ] Operacionalizar um pipeline DLT do raw para bronze
- [ ] Auditar e validar dados ingeridos e transformados

---

## üìö Recursos Adicionais

### Documenta√ß√£o Oficial
- [Databricks Documentation](https://docs.databricks.com/)
- [Unity Catalog Guide](https://docs.databricks.com/data-governance/unity-catalog/index.html)
- [Delta Lake Documentation](https://docs.delta.io/)

### Tutoriais Recomendados
- [Getting Started with Databricks](https://docs.databricks.com/getting-started/index.html)
- [Unity Catalog Quickstart](https://docs.databricks.com/data-governance/unity-catalog/get-started.html)
- [Delta Lake Quickstart](https://docs.delta.io/latest/quick-start.html)

### Comunidade
- [Databricks Community Forum](https://community.databricks.com/)
- [Stack Overflow - Databricks](https://stackoverflow.com/questions/tagged/databricks)

---

## ‚úÖ Checklist de Conclus√£o

- [ ] Scripts de ingest√£o (bitcoin e commodities) criados e funcionando
- [ ] Agendamentos configurados para rodar a cada 10 minutos
- [ ] Supabase criado, tabelas e seeds aplicados
- [ ] Rotinas SQL de transa√ß√µes rodando no Supabase
- [ ] Dados raw escritos no data lake simulado
- [ ] Pipeline DLT executado e tabelas bronze criadas
- [ ] Consultas de valida√ß√£o executadas nas tabelas bronze

---

## üöÄ Pr√≥ximos Passos

**Aula 2:** Modelagem, KPIs e Governan√ßa
- Modelagem l√≥gica/f√≠sica das tabelas silver
- KPIs iniciais e agrega√ß√µes
- Qualidade de dados (expectations) e governan√ßa
