# ðŸš€ ImersÃ£o Databricks â€” Jornada de Dados

Bem-vindo Ã  **ImersÃ£o Databricks**, um treinamento intensivo de **3 dias** criado pela **Jornada de Dados** para te levar do zero Ã  construÃ§Ã£o de pipelines completos na plataforma Databricks, dominando **Spark, Delta Lake e a arquitetura Lakehouse**.

---

## ðŸŽ¯ Objetivo

Capacitar vocÃª a construir pipelines de dados modernos para um **Lakehouse**, aplicando boas prÃ¡ticas de engenharia de dados com **scripts Python para ingestÃ£o de APIs** e **Fivetran para ingestÃ£o de bancos de dados**, usando **Databricks Free Edition** integrado ao **GitHub** para versionamento e governanÃ§a.

Durante a imersÃ£o, vocÃª vai:

- Criar sua conta gratuita no Databricks e integrar com o GitHub.  
- Entender a arquitetura **Bronze, Silver e Gold** dentro do Unity Catalog.  
- Ingerir dados de APIs com **scripts Python** e agendamento.  
- Ingerir dados de bancos relacionais usando **Fivetran**.  
- Aplicar transformaÃ§Ãµes e criar camadas analÃ­ticas.  
- Automatizar e visualizar seus pipelines no Databricks.

---

## ðŸ—“ï¸ Estrutura do Treinamento

### **Aula 1: IngestÃ£o via APIs com Python e Lakehouse (Rawâ†’Bronze)**
**Objetivo Principal:** Ingerir dados de 2 APIs (Bitcoin e commodities) com scripts Python, agendar execuÃ§Ãµes e organizar em camadas no Lakehouse.

**Objetivos EspecÃ­ficos:**
- Converter notebooks de ingestÃ£o em scripts Python executÃ¡veis e versionados.
- Agendar execuÃ§Ãµes (cron/Jobs) para coleta recorrente.
- Persistir dados na camada **raw** e promover para **bronze** (DLT).

**Principais TÃ³picos:** Python (requests/yfinance), arquivos raw, DLT, Databricks Jobs

---

### **Aula 2: IngestÃ£o de Bancos de Dados com Fivetran**
**Objetivo Principal:** Conectar bancos relacionais e ingerir dados para o Lakehouse usando **Fivetran**.

**Objetivos EspecÃ­ficos:**
- Configurar conectores Fivetran (ex.: Postgres/Supabase).
- Sincronizar dados para o storage/lake com Delta.
- Versionamento/monitoraÃ§Ã£o de cargas e qualidade bÃ¡sica.

**Principais TÃ³picos:** Fivetran, conectores, Lakehouse, Delta, monitoraÃ§Ã£o

---

### **Aula 3: AutomaÃ§Ã£o, Modelagem e PortfÃ³lio**
**Objetivo Principal:** Automatizar pipelines, modelar **Silver/Gold** e publicar resultados.

**Objetivos EspecÃ­ficos:**
- Desenvolver camadas **Silver** (refinadas) e **Gold** (curadas).
- Orquestrar pipelines e configurar SLAs/SLOs.
- Criar visualizaÃ§Ãµes/dashboards e consolidar portfÃ³lio.

**Principais TÃ³picos:** ETL/ELT, SQL no Databricks, Delta Lake, agendamento, visualizaÃ§Ã£o

---

## ðŸ§± Arquitetura de ReferÃªncia

O projeto segue o padrÃ£o de camadas **Medallion Architecture**:

```text
Sistema de Origem  â†’  Bronze (Raw)  â†’  Silver (Refined)  â†’  Gold (Curated)
                              |
                           Unity Catalog
                              |
                        GovernanÃ§a & Acesso
````

---

## ðŸ‘¥ PÃºblico-Alvo

Profissionais e estudantes de **dados, engenharia, BI e analytics** que desejam aprender na prÃ¡tica como criar pipelines reais e governados com **Spark e Databricks**.

---

## âš™ï¸ PrÃ©-Requisitos

* Conta gratuita no [Databricks Community Edition](https://community.cloud.databricks.com)
* Conta no [GitHub](https://github.com)
* Conta no [Fivetran](https://www.fivetran.com/) (trial) para a Aula 2
* Conhecimentos bÃ¡sicos de SQL e Python (opcional)

### ðŸ“º Tutorial: Como Criar sua Conta no Databricks

**Aprenda a criar sua conta no Databricks assistindo este vÃ­deo tutorial:**

ðŸŽ¥ **[Como Criar Conta no Databricks - Tutorial Completo](https://youtu.be/KJv1bZ6-gSY)**

Este vÃ­deo te guiarÃ¡ passo a passo para:
- Criar sua conta gratuita no Databricks Community Edition
- Configurar seu perfil e preferÃªncias iniciais
- Navegar pela interface do Databricks
- Configurar seu primeiro workspace

---

## ðŸ§  Tecnologias Utilizadas

* **Databricks**
* **Apache Spark**
* **Delta Lake**
* **Unity Catalog**
* **Fivetran**
* **Amazon S3 (simulado)**
* **Python / SQL**
* **Git e GitHub**

---

## ðŸ Resultados Esperados

Ao final da imersÃ£o, vocÃª serÃ¡ capaz de:

* Configurar um ambiente Databricks completo.
* Ingerir dados de APIs com scripts Python e de bancos com Fivetran.
* Aplicar boas prÃ¡ticas de governanÃ§a e versionamento.
* Entender a lÃ³gica da arquitetura **Lakehouse**.
* Criar pipelines reprodutÃ­veis e escalÃ¡veis.

---

## ðŸ“š CrÃ©ditos

Desenvolvido por **Luciano Vasconcelos** e o time da **Jornada de Dados**.
Mais informaÃ§Ãµes em: [https://jornadadedados.com.br](https://jornadadedados.com.br)

---

## ðŸ’¬ Contato

ðŸ“© **E-mail:** [contato@jornadadedados.com.br](mailto:contato@jornadadedados.com.br)
ðŸ’¼ **LinkedIn:** [Luciano Vasconcelos](https://linkedin.com/in/lucianovasconcelos)
ðŸ“º **YouTube:** [Jornada de Dados](https://youtube.com/@jornadadedados)

---

> â€œA engenharia de dados moderna comeÃ§a quando vocÃª entende que o dado Ã© o seu produto.â€
> â€” Jornada de Dados

```
